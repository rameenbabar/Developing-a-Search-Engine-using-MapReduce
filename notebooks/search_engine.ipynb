{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020caee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "#PREPROCESSING\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc651c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  #Check if the text is a string\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.strip() and token not in stop_words]\n",
    "        tokens = [token for token in tokens if token.strip()]\n",
    "        return tokens\n",
    "    else:\n",
    "        return []  #Return an empty list if the text is not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf53c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECTION_TEXT'] = df['SECTION_TEXT'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "#empty dictionary to store unique words and their term frequencies\n",
    "word_to_index = {}  \n",
    "\n",
    "#VOCABULARY\n",
    "#calculate term frequency for each word\n",
    "for idx, tokens_list in enumerate(df['SECTION_TEXT']):\n",
    "    #dictionary to store term frequencies for this section text\n",
    "    term_frequency = {}\n",
    "    for word in tokens_list:\n",
    "        if isinstance(word, str):\n",
    "            #Increment the term frequency\n",
    "            term_frequency[word] = term_frequency.get(word, 0) + 1\n",
    "            #If the word is not already in word_to_index dictionary, add it with its index\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = len(word_to_index)  #Assign a unique index to each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tokens_list in enumerate(df['SECTION_TEXT']):\n",
    "    term_frequency = {}\n",
    "    for word in tokens_list:\n",
    "        term_frequency[word] = term_frequency.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'vocab.txt'), 'w') as f:\n",
    "    f.write(\"Vocabulary:\\n\")\n",
    "    for word, idx in sorted(word_to_index.items(), key=lambda x: x[1]):\n",
    "        f.write(f\"{idx} {word}\\n\")\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ecf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'tf.txt'), 'w') as f:\n",
    "    for idx, tokens_list in enumerate(df['SECTION_TEXT']):\n",
    "        term_frequency = {}\n",
    "        for word in tokens_list:\n",
    "            term_frequency[word] = term_frequency.get(word, 0) + 1\n",
    "\n",
    "        f.write(f\"Term frequencies for Section {df['ARTICLE_ID'][idx]}:\\n\")\n",
    "        for word, frequency in term_frequency.items():\n",
    "            f.write(f\"({word_to_index[word]}, {frequency}), \")\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f6eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF or DOCUMENT FREQUENCY OF EACH WORD   \n",
    "#Initialize a dictionary to store document frequency for each word\n",
    "word_document_frequency = {}\n",
    "\n",
    "#Loop through each word in the vocabulary\n",
    "for word, index in word_to_index.items():\n",
    "    #Initialize document frequency count for this word\n",
    "    document_frequency = 0\n",
    "    #Iterate through each section text\n",
    "    for tokens_list in df['SECTION_TEXT']:\n",
    "        #If the word appears, increment the document frequency count\n",
    "        if word in tokens_list:\n",
    "            document_frequency += 1\n",
    "    #Store the document frequency count\n",
    "    word_document_frequency[word] = document_frequency\n",
    "    \n",
    "print(\"Document Frequencies:\")\n",
    "for word, frequency in word_document_frequency.items():\n",
    "    print(f\"({word_to_index[word]}, {frequency})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3720587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF/IDF WEIGHTS\n",
    "#Calculate weights of each word\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'tfidf.txt'), 'w') as f:\n",
    "    f.write(\"Weights for each word in the document:\\n\")\n",
    "    for idx, tokens_list in enumerate(df['SECTION_TEXT']):\n",
    "        term_frequency = {}\n",
    "        for word in tokens_list:\n",
    "            term_frequency[word] = term_frequency.get(word, 0) + 1\n",
    "\n",
    "        f.write(f\"Weights for Section {df['ARTICLE_ID'][idx]}:\\n\")\n",
    "        for word, frequency in term_frequency.items():\n",
    "            word_index = word_to_index[word]\n",
    "            document_frequency = word_document_frequency[word]\n",
    "            # Calculate the weight by dividing term frequency by document frequency\n",
    "            weight = frequency / document_frequency\n",
    "            f.write(f\"({word_index}, {weight:.5f}), \")\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef55918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VECTOR SPACE MODEL\n",
    "\n",
    "#VECTOR LIST\n",
    "\n",
    "#Initialize a list to store weights for each section text\n",
    "section_weights = []\n",
    "\n",
    "#Iterate through each section text\n",
    "for tokens_list in df['SECTION_TEXT'][:10]:  \n",
    "    term_frequency = {}\n",
    "    section_weight = [0] * len(word_to_index)  #Initialize weights list for this section\n",
    "    \n",
    "    for word in tokens_list:\n",
    "        term_frequency[word] = term_frequency.get(word, 0) + 1\n",
    "    \n",
    "    #Calculate weights for each word\n",
    "    for word, frequency in term_frequency.items():\n",
    "        if word in word_to_index:  #Check if the word exists in the vocabulary\n",
    "            word_index = word_to_index[word]\n",
    "            document_frequency = word_document_frequency[word]\n",
    "            weight = frequency / document_frequency\n",
    "            weight = round(weight, 5)\n",
    "            section_weight[word_index] = weight  #Update weight for this word in the section\n",
    "    \n",
    "    #Append the weights to the section_weights list\n",
    "    section_weights.append(section_weight)\n",
    "\n",
    "for idx, weights in enumerate(section_weights):\n",
    "    with open(os.path.join(output_dir, 'tfidf.txt'), 'a') as f:\n",
    "        f.write(f\"Weights for Section {df['ARTICLE_ID'][idx]}:\\n\")\n",
    "        f.write(\", \".join([f\"({i}, {round(w, 5)})\" for i, w in enumerate(weights) if w > 0]))\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY LIST\n",
    "\n",
    "# Initialize a query list with zeros\n",
    "query_list = [0] * len(word_to_index)\n",
    "\n",
    "#Define the sentence to search for\n",
    "query_sentence = input(\"Enter a sentence: \")\n",
    "query_words = preprocess_text(query_sentence)\n",
    "\n",
    "#Initialize a list to store the relevance scores for each word in the query sentence\n",
    "word_relevance_scores = []\n",
    "\n",
    "#Iterate through each word in the query sentence\n",
    "for query_word in query_words:\n",
    "    #Check if the query word exists in the vocabulary\n",
    "    if query_word in word_to_index:\n",
    "        query_word_index = word_to_index[query_word]\n",
    "        \n",
    "        #Initialize a counter to keep track of the number of section texts containing the query term \n",
    "        document_counter = 0\n",
    "        \n",
    "        #Iterate through each document\n",
    "        for tokens_list in df['SECTION_TEXT']:\n",
    "            #Check if the query word exists in the document\n",
    "            if query_word in tokens_list:\n",
    "                document_counter += 1  #increment the document counter\n",
    "                \n",
    "                #Calculate term frequency for the query word\n",
    "                term_frequency = tokens_list.count(query_word)\n",
    "                \n",
    "                #Calculate the weight of the query word\n",
    "                document_frequency = word_document_frequency[query_word]\n",
    "                weight = term_frequency / document_frequency\n",
    "                \n",
    "                #Update the corresponding index in the query list with the weight\n",
    "                query_list[query_word_index] += weight\n",
    "        \n",
    "        #If the query word exists in at least one section text\n",
    "        if document_counter > 0:\n",
    "            # Calculate the average weight of the query word across all section texts\n",
    "            average_weight = query_list[query_word_index] / document_counter\n",
    "            #Update the query list at the index corresponding to the query word\n",
    "            query_list[query_word_index] = average_weight\n",
    "    \n",
    "    word_relevance_scores.append(query_list[query_word_index])\n",
    "\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'query_list.txt'), 'w') as f:\n",
    "    f.write(\"Query List:\\n\")\n",
    "    f.write(\", \".join(map(str, query_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29158716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a list to store the relevance scores for each section\n",
    "section_relevance_scores = []\n",
    "\n",
    "#Iterate through each section weight vector\n",
    "for section_weight in section_weights:\n",
    "    section_total_relevance_score = 0\n",
    "    \n",
    "    #Iterate through each word in the query sentence\n",
    "    for query_word_index, word_relevance in enumerate(word_relevance_scores):\n",
    "        #Multiply the weight of the word in the section_weight vector with the weight of the same word in the query list\n",
    "        word_relevance = section_weight[query_word_index] * word_relevance\n",
    "        \n",
    "        #Add the relevance of this word to the total relevance score for this section\n",
    "        section_total_relevance_score += word_relevance\n",
    "    \n",
    "    section_relevance_scores.append(section_total_relevance_score)\n",
    "\n",
    "for idx, score in enumerate(section_relevance_scores):\n",
    "    if score > 0:\n",
    "        section_text = ' '.join(df['SECTION_TEXT'][idx])  \n",
    "        with open(os.path.join(output_dir, 'relevance.txt'), 'a') as f:\n",
    "            f.write(f\"\\nRelevance score for Section {df['ARTICLE_ID'][idx]}: {score}\\n\")\n",
    "            f.write(f\"Section text content: {section_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
